{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Value Iteration\n",
    "This notebook serves as a tutorial for using the DiscreteValueIteration.jl module, and aims to illustrate:\n",
    "\n",
    "* How to correctly define the functions used by the module\n",
    "* How to use the basic value iteration solver\n",
    "* How to use the parallel value iteration solver\n",
    "* How to work with the Policy and Solver types provided by the module\n",
    "\n",
    "This module is tailored to dealing with sparse MDPs that have large state spaces. To that end, it requires the user to define the functions that represent the rewards, and the dynamics of the MDP as opposed to explicitly creating the reward and transition matrices. This tutrial will use a random MDP as an example. A more compelx example can be found in GridWorlds.jl. \n",
    "\n",
    "DiscreteValueIteration.jl requires DiscreteMDPs.jl, in which the API for handling discrete MDPs is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Note\n",
    "The easiest way to get the parallel solver to work is to add the following command before importing any of the modules you plan to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Any,1}:\n",
       " 2\n",
       " 3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first add the number of processors you would like to use in the parallel solver\n",
    "addprocs(int(CPU_CORES/2)); # CPU_CORES includes hyperthreaded cores, divide by two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the MDP\n",
    "This section defines a random MDP. It uses the functions and abstract types defined in DiscreteMDPs. To avoid @everywhere declarations, wrap your MDP code in a module, and import it after calling addprocs. \n",
    "\n",
    "To use the solver, the following functions must be defined:\n",
    "```julia\n",
    "states(mdp::YourMDP) # returns an iterator over MDP states\n",
    "actions(mdp::YourMDP) # returns an iterator over MDP actions\n",
    "numStates(mdp::YourMDP) # returns the number of states\n",
    "numActions(mdp::YourMDP) # returns the number of actions\n",
    "nextStates(mdp::YourMDP, state, action) # returns arrays neighboring states and their probabilities \n",
    "reward(mdp::YourMDP, state, action) # returns the immediate reward of the state-action pair\n",
    "```\n",
    "\n",
    "The reward function is: $R(s,a)$. The nextStates function is similar to a sparse transition function. For a given pair $(s, a)$, it returns the neighboring states $s'$ and their transition probabilities $T(s'\\mid s, a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "###################### RandomMDPs.jl ############################\n",
    "#################################################################\n",
    "\n",
    "# The following is defined in RandomMDPs.jl in the test directory\n",
    "\n",
    "#=\n",
    "module RandomMDPs\n",
    "\n",
    "export RandomMDP\n",
    "export states, actions\n",
    "export numStates, numActions\n",
    "export reward, nextStates\n",
    "\n",
    "using DiscreteMDPs\n",
    "\n",
    "import DiscreteMDPs.DiscreteMDP\n",
    "import DiscreteMDPs.reward\n",
    "import DiscreteMDPs.nextStates\n",
    "import DiscreteMDPs.states\n",
    "import DiscreteMDPs.actions\n",
    "import DiscreteMDPs.numStates\n",
    "import DiscreteMDPs.numActions\n",
    "\n",
    "\n",
    "type RandomMDP <: DiscreteMDP\n",
    "\n",
    "    nStates::Int64\n",
    "    nActions::Int64\n",
    "    nNeighbors::Int64 # numbers of neighbors per state\n",
    "    randomSeed::Int64\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function states(mdp::RandomMDP)\n",
    "    # returns an iterator over the states\n",
    "    return 1:mdp.nStates\n",
    "end\n",
    "\n",
    "function actions(mdp::RandomMDP)\n",
    "    # returns an iterator over the actions\n",
    "    return 1:mdp.nActions\n",
    "end\n",
    "\n",
    "function numStates(mdp::RandomMDP)\n",
    "    # returns the number of states\n",
    "    return mdp.nStates\n",
    "end\n",
    "\n",
    "function numActions(mdp::RandomMDP)\n",
    "    # returns the number of actions\n",
    "    return mdp.nActions\n",
    "end\n",
    "\n",
    "\n",
    "function reward(mdp::RandomMDP, state::Int64, action::Int64)\n",
    "    # reward is generated using the random seed and the action+state sum\n",
    "    seed = state+action+mdp.randomSeed\n",
    "    srand(seed)\n",
    "\n",
    "    r = rand()\n",
    "    return r\n",
    "end\n",
    "\n",
    "\n",
    "function nextStates(mdp::RandomMDP, state::Int64, action::Int64)\n",
    "    # returns the nighboring states and their probabilities\n",
    "    randomSeed = mdp.randomSeed\n",
    "    nNeighbors = mdp.nNeighbors\n",
    "    nStates    = numStates(mdp)\n",
    "\n",
    "    # these can be pre-allocated in the MDP type\n",
    "    states = zeros(Int64, nNeighbors)\n",
    "    probs  = zeros(nNeighbors)\n",
    "\n",
    "    # set a unique seed for each (state, action) pair\n",
    "    seed = state+action+mdp.randomSeed\n",
    "    srand(seed)\n",
    "    for i = 1:nNeighbors\n",
    "        # random neighbors and probabilities\n",
    "        states[i] = rand(1:nStates)\n",
    "        probs[i]  = rand()\n",
    "    end\n",
    "\n",
    "    # normalize\n",
    "    norm = sum(probs)\n",
    "    for i = 1:nNeighbors\n",
    "        probs[i] = probs[i] / norm\n",
    "    end\n",
    "    return states, probs\n",
    "end\n",
    "\n",
    "end # module\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Module\n",
    "\n",
    "We now have everything we need to run the solver. First, we will run through an example of using the serial solver. If you plan to use the parallel solver, make sure you include the addprocs command before defining or importing the MDP function definitions, and the DiscreteValueIteration module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomMDP(500,5,5,1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load the module\n",
    "using DiscreteValueIteration\n",
    "using RandomMDPs\n",
    "\n",
    "# create the MDP type\n",
    "mdp = RandomMDP(500, 5, 5, 1) # input args: state size, action size, neighbors, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serial Solver\n",
    "The serial solver type has a number of optional input arguments:\n",
    "* maxIterations: the maximum number of iterations in the DP loop (default is 1000)\n",
    "* tolerance: the Bellman residual (default is 1e-3)\n",
    "* gaussSiedel: flag for Gauss-Siedel value iteration (default is true)\n",
    "* includeV: flag for returning the utility function (defualt is true)\n",
    "* includeQ: flag for returning the Q-matrix (default is true)\n",
    "* includeA: flag for returning the policy (default is true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SerialSolver(50,0.001,true,true,true,true)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxIterations = 50 # maximum number of iterations in the DP loop\n",
    "tolerance = 1e-2   # Bellman residual\n",
    "gs = true          # Gauss-Siedel falg\n",
    "includeV = true    # return utility flag\n",
    "includeQ = true    # return Q-matrix flag\n",
    "includeA = false   # return policy flag\n",
    "solver = SerialSolver(maxIterations=maxIterations, tolerance=tolerance, gaussSiedel=gs,\n",
    "                      includeV=includeV, includeQ=includeQ, includeA=includeA)\n",
    "solver = SerialSolver(maxIterations=50) # this also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscretePolicy{Float64,Float64,Int64}([101.49,101.49,101.49,101.459,101.459,101.413,101.47,101.47,101.47,101.47  …  103.349,103.349,103.378,103.378,103.378,103.378,103.378,103.203,102.971,102.971],5x500 Array{Float64,2}:\n",
       " 101.407  100.932  101.49   101.047  …  103.378  103.203  102.75   102.971\n",
       " 100.932  101.49   101.047  101.459     103.203  102.75   102.971  102.187\n",
       " 101.49   101.047  101.459  100.09      102.75   102.971  102.187  101.759\n",
       " 101.047  101.459  100.09   101.413     102.971  102.187  101.759  102.432\n",
       " 101.459  100.09   101.413  100.22      102.187  101.759  102.432  102.966,[3,2,1,2,1,2,5,4,3,2  …  3,2,5,4,3,2,1,1,2,1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = solve(solver, mdp); # solve using value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following API gives access to utility, Q-matrix and policy values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1\n",
    "a = 1\n",
    "u  = value(policy, s) # expected optimal value for state s\n",
    "q  = value(policy, s, a) # expected value for state-action pair\n",
    "ap = action(policy, s) # action that maximizes the expected utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Solver\n",
    "The parallel solver has two additional input arguments:\n",
    "* numProcessors: the number of processors to be used by the solver (this argument is required)\n",
    "* stateOrdering: used for backwards induction value iteration to indicate the state ordering (optinal argument)\n",
    "The stateOrder is an Array of UnitRange types (iterators) over the states that need to be updated before other states. In the example below, the 1:250 chunk will be updated before the 251:500 chunk at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelSolver(2,None[],50,0.001,true,true,true,true)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numProcs   = 2 # numProcs should not exceed the value returned by nprocs()\n",
    "stateOrder = {1:250,251:500} # default ordering is {[1,numStates]}\n",
    "solver = ParallelSolver(numProcs, stateOrder=stateOrder, maxIterations=maxIterations,\n",
    "                        tolerance=tolerance, gaussSiedel=gs,\n",
    "                        includeV=includeV, includeQ=includeQ, includeA=includeA)\n",
    "solver = ParallelSolver(numProcs, maxIterations=50) # this also works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscretePolicy{Float64,Float64,Int64}([101.49,101.49,101.49,101.459,101.459,101.413,101.47,101.47,101.47,101.47  …  103.349,103.349,103.378,103.378,103.378,103.378,103.378,103.203,102.971,102.971],5x500 SharedArray{Float64,2}:\n",
       " 101.407  100.932  101.49   101.047  …  103.378  103.203  102.75   102.971\n",
       " 100.932  101.49   101.047  101.459     103.203  102.75   102.971  102.187\n",
       " 101.49   101.047  101.459  100.09      102.75   102.971  102.187  101.759\n",
       " 101.047  101.459  100.09   101.413     102.971  102.187  101.759  102.432\n",
       " 101.459  100.09   101.413  100.22      102.187  101.759  102.432  102.966,[3,2,1,2,1,2,5,4,3,2  …  3,2,5,4,3,2,1,1,2,1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = solve(solver, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same API can be used to access the utility, Q-matrix, and policy values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 1\n",
    "a = 1\n",
    "u  = value(policy, s) # expected optimal value for state s\n",
    "q  = value(policy, s, a) # expected value for state-action pair\n",
    "ap = action(policy, s) # action that maximizes the expected utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.4",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "name": "julia",
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
